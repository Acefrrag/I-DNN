####### FILESYSTEM DESCRIPTION

The folder "files" contains generated DNN training data, fixed-point representation data and VHDL architecture generated by the script.
This folder contains python code and input files to compute a layer's output and compare it with the layer output from the VHDL implementation.


###### DATA GENERATION #########
The DNN data generation involves different stages:

-Training of the DNN
-Generation of fixed point data
-Generation of the VHDL architecture
-Uploading of data in the DNN VHDL architecture.

DNN Training
The first step is to generate the weights and biases for a given DNN topology.

The script to perform this task is train_DNN.py
When executing the script the user has to provide the number of layers, the number of neurons per layer, and the activation function type.
After this stage a file Weights_ann_Biases.txt containing the DNN parameters is created in files/weight_n_biases/training_<dd-mm-yy>_<hh-mm-ss>

FIXED-POINT DATA GENERATION

This involves the creation of several files, which are:

-Sigmoid ROM content, the user must run genSigmoid.py script. The file is SigContent.mif in the sigmoid folder
-Weight and biases ROMs content, this is included in the trainNN.py script. The files are stored in the folder weight and the folder biases.
-Handwritten number image input content, the user must run gen_VHDL_test_data.py script. 

VHDL Architecture generation

This step includes the generation of the DNN top level as well the DNN_package containing the DNN parameters.

This is included in the trainNN.py script.




######TESTBENCHING
#NEURON
######################NEURON FILE SYSTEM###################
############################INPUT_DATA############################
The input data files should be having the following format:

<symbolical_name>_<numberofinputs>_<inputwidth>_<inputIntWidth>.txt

Example:

data_input_file1_30_32_16.txt

This file will contain a number of 30 inputs, the overall
size of the input is 32 and the fractional part size is 16.

########################WEIGHT_DATA###############################
The weight data files should be having the following
format:

<symbolical_name>_<numberofweights>_<weightsWidth>_<weightsIntWidth>.txt

Example

data_weight_file1_30_32_16.txt

This file will contain a number of 30 weights, the overall
size of the weight is 32 and the fractional part size is 16


#############################BIAS##################################

For bias there is only one sample in the file, therefore the format will be

<symbolical_name>_<biasWidth>_<biasIntWidth>.txt

The bias has a size which is the sum of the sizes of the input and weight.

#LAYER
###################LAYER FILE SYSTEM###################
For the layer the filesystem is organized in this way
Inside the layer data parent directory there must 3 other directories
-weights
-inputs
-biases
####WEIGHT FILE FORMAT########
The weights filenames are of the type w_<layer>_<neuron>.mif, where
<layer> is the number of the layer, while <neuron> is the neuron number
within the layer.
Since the single layer is just one, <layer> value is inherited from the DNN generator
python script.
###


The MNIST package contains dataset for training, validation and testing of the DNN.

The training dataset contains a set of 50000 handwritten number images.

The validation dataset and the test dataset contains a set of 10000 handwritten numbers.

The test dataset is supposed to be used to perform the testing after the DNN has been trained and validated.
The inputs are fed into the DNN architecture during the testbench.


#DNN
##################DNN TESTBENCHING#############################
To test the DNN the python DNN_tester.py script is used.