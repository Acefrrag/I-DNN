TRAINING REPORT - 04-16-23_22-35-53
 
Number of hidden layers : 4
Neurons per layer: [30, 25, 15, 10]
Activation function type: ReLU
Training data size: 50000
Training epochs: 1
Training batch size:100
Training learning rate: eta =10
Training regularitation value: lambda =0
Evaluation data size: 10000
Evaluation Accuracies per epoch: 
Epoch:0 Accuracy: 1064/10000
