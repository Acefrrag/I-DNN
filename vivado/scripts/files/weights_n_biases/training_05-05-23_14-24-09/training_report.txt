TRAINING REPORT - 05-05-23_14-24-09
 
Number of hidden layers : 12
Neurons per layer: [100, 60, 50, 45, 35, 30, 30, 25, 20, 15, 15, 10]
Activation function type: ReLU
Training data size: 50000
Training epochs: 1
Training batch size:100
Training learning rate: eta =10
Training regularitation value: lambda =0
Evaluation data size: 10000
Evaluation Accuracies per epoch: 
Epoch:0 Accuracy: 991/10000
