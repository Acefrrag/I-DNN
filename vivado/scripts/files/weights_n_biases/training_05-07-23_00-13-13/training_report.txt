TRAINING REPORT - 05-07-23_00-13-13
 
Number of hidden layers : 8
Neurons per layer: [90, 65, 50, 25, 20, 15, 15, 10]
Activation function type: ReLU
Training data size: 50000
Training epochs: 1
Training batch size:100
Training learning rate: eta =10
Training regularitation value: lambda =0
Evaluation data size: 10000
Evaluation Accuracies per epoch: 
Epoch:0 Accuracy: 991/10000
